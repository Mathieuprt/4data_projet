# Projet 4Data

## Objectif du projet

Ce projet impl√©mente une pipeline de donn√©es compl√®te autour des matchs ATP Tennis (2000‚Äì2023) √† l'aide de **Dagster** pour l'orchestration, **DBT** pour la transformation, et **DuckDB** comme moteur de stockage. L'objectif est de :
- Collecter des donn√©es depuis Kaggle (dataset ATP)
- Les filtrer et charger par ann√©e (pipeline partitionn√©)
- Les transformer via des mod√®les DBT
- Exporter les r√©sultats pour des outils d'analyse comme Power BI

## Source de donn√©es 

https://www.kaggle.com/datasets/dissfya/atp-tennis-2000-2023daily-pull

## Choix de conception

### Pipeline partitionn√© par ann√©e
Chaque ex√©cution traite les donn√©es d‚Äôune **ann√©e sp√©cifique**, facilitant la r√©ex√©cution, la maintenance et l‚Äôanalyse temporelle.

### Stockage dans DuckDB
Nous utilisons DuckDB pour sa l√©g√®ret√©, sa compatibilit√© avec DBT. N√©anmoins, Power BI n'int√©grant pas de connecteurs natifs avec DuckDB, un script Python a √©t√© mis en place pour exporter les donn√©es en csv

### Outils utilis√©s
- **Dagster** : orchestration des assets et jobs
- **DBT** : transformation SQL modulaire et testable
- **DuckDB** : base de donn√©es fichier performante
- **Power BI** : visualisation finale (via fichiers `.csv` ou ODBC)


## √âtapes d'installation

### Pr√©requis

- Python 3.9+
- `pip` ou `venv`
- Compte Kaggle avec API key (`kaggle.json`)

### Installation

```bash
cd 4data_projet
python -m venv venv
venv\Scripts\activate        # ou source venv/bin/activate sur Mac/Linux
cd ../..
pip install -r requirements.txt
```

### Configuration

Placer le fichier `kaggle.json` dans :
```
C:\Users\<user>\.kaggle
```

---

## Ex√©cution de la pipeline

### 1. Lancer Dagster

```bash
dagster dev -f project_dagster/project_dagster
```

### 2. Depuis l'interface Dagster

- Aller dans **Jobs > atp_elt_job**
- Cliquer sur **Launch Partition Backfill**
- Choisir une ann√©e, puis lancer

### 3. En ligne de commande

```bash
dagster job launch -j atp_elt_job -p 2023
```

### 4. Export CSV (optionnel)

```bash
python project_dagster/scripts/export_all_tables.py
```

Puis connecter les fichiers `.csv` √† Power BI.

---

## üìà Monitoring & Logs

- Suivi en direct via Dagster UI (`Runs`, `Assets`, `Logs`)

---

## Planification automatique

La pipeline est planifi√©e pour s‚Äôex√©cuter **tous les jours √† 18h (Europe/Paris)** via `build_schedule_from_partitioned_job`.

---

## Tests

Lancer les tests unitaires avec :

```bash
pytest
```

Les tests se trouvent dans :
```
project_dagster/project_dagster_tests/
```
